{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/alpp/blob/main/colab/alpp_saving_and_loading_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsDuRWvnhlW8"
      },
      "source": [
        "# [Saving And Loading Models - PyTorch Beginner 17](https://www.python-engineer.com/courses/pytorchbeginner/17-saving-and-loading/)\n",
        "\n",
        "- `torch.save` can save a model, tensor, or dictionary.\n",
        "- `torch.load` loads the saved model, tensor, or dictionary.\n",
        "- `model.load_state_dict` can be used for restoring the saved parameters `model.state_dict()`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assume we have defined a class `Model` and created its instance `model`."
      ],
      "metadata": {
        "id": "bCFHZOnOQP65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "\n",
        "# train your model...\n"
      ],
      "metadata": {
        "id": "_CYdTylmQIfj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRfZo3BurGfs"
      },
      "source": [
        "## Method 1: save and load entire model\n",
        "\n",
        "- `torch.save(model, FILE)`\n",
        "- `loaded_model = torch.load(FILE)`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    print(param)\n",
        "\n",
        "FILE = \"model.pth\"\n",
        "torch.save(model, FILE)\n",
        "\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbyQhDIgQ-7w",
        "outputId": "af295125-3c35-40ff-b6fa-0c4e9b3715d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1668,  0.3707, -0.1982, -0.0161, -0.1367,  0.0242]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2696], requires_grad=True)\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1823 Jul 18 14:23 model.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"model.pth\"\n",
        "loaded_model = torch.load(FILE)\n",
        "loaded_model.eval()\n",
        "\n",
        "for param in loaded_model.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uipWVt4CYbMT",
        "outputId": "4db595a8-024d-493d-fa8f-46c650c1cece"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1668,  0.3707, -0.1982, -0.0161, -0.1367,  0.0242]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2696], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. If you wish to resuming training, call model.train() to ensure these layers are in training mode."
      ],
      "metadata": {
        "id": "pHFCEbxQfdp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Method 2: save and load only state dict\n",
        "\n",
        "` model.state_dict()` is a set of all model weights.\n",
        " - `torch.save(model.state_dict(), FILE)`\n",
        " - `loaded_model.load_state_dict(torch.load(FILE))`"
      ],
      "metadata": {
        "id": "O-w8TK9MOQNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict())\n",
        "\n",
        "FILE = \"model_dict.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSRpwVuGRcFo",
        "outputId": "223068ff-a10a-4348-cdde-f67329ffc1ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.1668,  0.3707, -0.1982, -0.0161, -0.1367,  0.0242]])), ('linear.bias', tensor([0.2696]))])\n",
            "total 12\n",
            "-rw-r--r-- 1 root root 1139 Jul 18 14:41 model_dict.pth\n",
            "-rw-r--r-- 1 root root 1823 Jul 18 14:23 model.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"model_dict.pth\"\n",
        "\n",
        "# create a model with the same archtecture before loading the weights\n",
        "loaded_model = Model(n_input_features=6)\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(FILE))\n",
        "loaded_model.eval()\n",
        "\n",
        "print(loaded_model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDF6AzwdcEoV",
        "outputId": "0fc10e2d-04a6-4110-f0f8-2713a40d36aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.1668,  0.3707, -0.1982, -0.0161, -0.1367,  0.0242]])), ('linear.bias', tensor([0.2696]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load an optimizer\n",
        "\n",
        "An optimizer has `state_dict`, so it can be saved and loaded in the same way as a model."
      ],
      "metadata": {
        "id": "_Uo6so6ZfyUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "print(optimizer.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWPD7BRSgKbf",
        "outputId": "55ec65a8-50e5-49e1-8d08-b6a18534a423"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load a checkpoint as a dictionary of model and optimzer states in training"
      ],
      "metadata": {
        "id": "Uc9u3jgZgvUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "\"epoch\": 90,\n",
        "\"model_state\": model.state_dict(),\n",
        "\"optim_state\": optimizer.state_dict()\n",
        "}\n",
        "\n",
        "FILE = \"checkpoint.pth\"\n",
        "torch.save(checkpoint, FILE)\n",
        "\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHvU0i7pfwac",
        "outputId": "a75e5d5b-f530-4f87-eb69-2b87f4b11b3a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "-rw-r--r-- 1 root root 1395 Jul 18 15:06 checkpoint.pth\n",
            "-rw-r--r-- 1 root root 1139 Jul 18 14:41 model_dict.pth\n",
            "-rw-r--r-- 1 root root 1823 Jul 18 14:23 model.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"checkpoint.pth\"\n",
        "\n",
        "# create a model and optimizer with the same archtecture before loading the states\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
        "\n",
        "loaded_checkpoint = torch.load(FILE)\n",
        "epoch = loaded_checkpoint['epoch']\n",
        "loaded_model.load_state_dict(loaded_checkpoint['model_state'])\n",
        "loaded_optimizer.load_state_dict(loaded_checkpoint['optim_state'])\n",
        "\n",
        "print(loaded_optimizer.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yy2WcZPiZqN",
        "outputId": "fd09fb90-4fcb-4ee5-af40-11f09d0d909a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU/GPU mapping\n",
        "\n",
        "- use `map_location` option in `load_state_dict()`"
      ],
      "metadata": {
        "id": "KR_GxnlBljZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Save on GPU, Load on CPU\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(model.state_dict())   # see device='cuda:0'\n",
        "\n",
        "FILE = \"model_state_in_GPU.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "!ls -l\n",
        "\n",
        "device = torch.device('cpu')\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_model.load_state_dict(torch.load(FILE, map_location=device))\n",
        "loaded_model.eval()\n",
        "\n",
        "print(loaded_model.state_dict())   # no see device='cuda:0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1N-CKNiluwi",
        "outputId": "393214ac-dfbe-49f1-e740-7171d07519d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.1622,  0.2330,  0.0682,  0.3841,  0.3969,  0.3914]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([-0.0846], device='cuda:0'))])\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1171 Jul 18 15:28 model_state_in_GPU.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n",
            "OrderedDict([('linear.weight', tensor([[-0.1622,  0.2330,  0.0682,  0.3841,  0.3969,  0.3914]])), ('linear.bias', tensor([-0.0846]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Save on GPU, Load on GPU\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(model.state_dict())   # see device='cuda:0'\n",
        "\n",
        "FILE = \"model_state_in_GPU.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "!ls -l\n",
        "\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_model.load_state_dict(torch.load(FILE))  # No map_location required\n",
        "loaded_model.to(device)\n",
        "print(loaded_model.state_dict())    # see device='cuda:0' again\n",
        "\n",
        "# Note: Be sure to use the .to(torch.device('cuda')) function\n",
        "# on all model inputs, too!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUtj72DpnLYz",
        "outputId": "9e3cf9d9-51f6-4bb8-abef-af86dda3e20a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.3456,  0.2816, -0.0176, -0.1030,  0.0768,  0.1350]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([0.1201], device='cuda:0'))])\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1171 Jul 18 15:37 model_state_in_GPU.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n",
            "OrderedDict([('linear.weight', tensor([[-0.3456,  0.2816, -0.0176, -0.1030,  0.0768,  0.1350]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([0.1201], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Save on CPU, Load on GPU\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "print(model.state_dict())   # no see device='cuda:0'\n",
        "\n",
        "FILE = \"model_state_in_CPU.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "!ls -l\n",
        "\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_model.load_state_dict(torch.load(FILE, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
        "device = torch.device(\"cuda\")\n",
        "loaded_model.to(device)     # be sure to call this to convert the model's parameter tensors to CUDA tensors\n",
        "print(loaded_model.state_dict())    # see device='cuda:0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRk9TcF3otSr",
        "outputId": "cd30010a-b4a4-4b49-8209-500e4b36806e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.1131,  0.3753, -0.0917, -0.0258, -0.3139,  0.2568]])), ('linear.bias', tensor([0.1556]))])\n",
            "total 12\n",
            "-rw-r--r-- 1 root root 1171 Jul 18 15:42 model_state_in_CPU.pth\n",
            "-rw-r--r-- 1 root root 1171 Jul 18 15:37 model_state_in_GPU.pth\n",
            "drwxr-xr-x 1 root root 4096 Jul 14 13:31 sample_data\n",
            "OrderedDict([('linear.weight', tensor([[-0.1131,  0.3753, -0.0917, -0.0258, -0.3139,  0.2568]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([0.1556], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's all!"
      ],
      "metadata": {
        "id": "_63_WUeDqzOW"
      }
    }
  ]
}